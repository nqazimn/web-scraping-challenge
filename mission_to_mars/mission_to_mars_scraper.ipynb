{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up urls to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up dictionary of all Mars urls to be scraped\n",
    "url_dict = {\"NASA\":\"https://mars.nasa.gov/news/\",\\\n",
    "           \"JPL\":\"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\",\\\n",
    "           \"Twitter\":\"https://twitter.com/marswxreport?lang=en\",\\\n",
    "           \"space-facts\":\"https://space-facts.com/mars/\",\\\n",
    "           \"astrogeology\":\"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape NASA and extract latest news _title_ and _text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url = url_dict[\"NASA\"])\n",
    "\n",
    "# Wait for 20 sec while the page loads\n",
    "time.sleep(20)\n",
    "\n",
    "soup = bs(response.text, \"html.parser\")\n",
    "\n",
    "# print(soup.prettify())\n",
    "\n",
    "results = soup.find_all(\"div\", class_ = \"slide\")\n",
    "\n",
    "mars_latest_news = []\n",
    "\n",
    "for result in results:    \n",
    "    try:    \n",
    "        news_title = result.find(\"div\", class_ = \"content_title\").text\n",
    "        news_description = result.find(\"div\", class_ = \"rollover_description_inner\").text\n",
    "\n",
    "        news_dict = {'title':news_title.strip(),\\\n",
    "                    'description':news_description.strip()}\n",
    "\n",
    "        mars_latest_news.append(news_dict)\n",
    "    except AttributeError as e:\n",
    "        print(e)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Virginia Middle School Student Earns Honor of Naming NASA's Next Mars Rover\n",
      "Descirption: NASA chose a seventh-grader from Virginia as winner of the agency's \"Name the Rover\" essay contest. Alexander Mather's entry for \"Perseverance\" was voted tops among 28,000 entries.\n",
      "\n",
      "Title: NASA Prepares for Moon and Mars With New Addition to Its Deep Space Network\n",
      "Descirption: Robotic spacecraft will be able to communicate with the dish using radio waves and lasers.\n",
      "\n",
      "Title: NASA Administrator Statement on Moon to Mars Initiative, FY 2021 Budget\n",
      "Descirption: Jim Bridenstine addresses NASA's ambitious plans for the coming years, including Mars Sample Return.\n",
      "\n",
      "Title: NASA's Mars 2020 Rover Closer to Getting Its Name\n",
      "Descirption: 155 students from across the U.S. have been chosen as semifinalists in NASA's essay contest to name the Mars 2020 rover, and see it launch from Cape Canaveral this July.\n",
      "\n",
      "Title: NASA Invites Students to Name Mars 2020 Rover\n",
      "Descirption: Through Nov. 1, K-12 students in the U.S. are encouraged to enter an essay contest to name NASA's next Mars rover.\n",
      "\n",
      "Title: NASA's Curiosity Mars Rover Finds a Clay Cache\n",
      "Descirption: The rover recently drilled two samples, and both showed the highest levels of clay ever found during the mission.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scanity-check\n",
    "for news in mars_latest_news:\n",
    "    print('Title: '+ news['title'])\n",
    "    print('Descirption: '+news['description'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape JPL and extract latest Mars featured image (using Splinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the browser and visit JPL page\n",
    "\n",
    "browser = Browser('chrome', headless=False)\n",
    "browser.visit(url_dict['JPL'])\n",
    "\n",
    "# Wait for 20 sec while the page loads\n",
    "time.sleep(20)\n",
    "\n",
    "# scrape the page\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "featured_image = soup.find(\"li\", class_=\"slide\").a['data-fancybox-href']\n",
    "\n",
    "# create full-url to high-res image\n",
    "featured_image_url = url_dict['JPL'].split('/spaceimages')[0]+featured_image\n",
    "\n",
    "browser.quit()\n",
    "# sanity-check\n",
    "# browser.visit(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrape Twitter to extract Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url = url_dict[\"Twitter\"])\n",
    "\n",
    "# Wait for 20 sec while the page loads\n",
    "time.sleep(20)\n",
    "\n",
    "soup = bs(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sol 457 (2020-03-10) low -95.7ºc (-140.3ºf) high -9.1ºc (15.6ºf); winds from the sse at 6.5 m/s (14.5 mph) gusting to 21.0 m/s (46.9 mph); pressure at 6.30 hpa\n"
     ]
    }
   ],
   "source": [
    "# using re: Regular Expression matching (source:https://docs.python.org/3/library/re.html)\n",
    "# since the weather reports always start with 'Insight sol' on the Twitter page\n",
    "\n",
    "weather_reports = soup(text=re.compile(r'InSight sol'))\n",
    "\n",
    "try:\n",
    "    # only select the latest weather report\n",
    "    mars_weather = str(weather_reports[0])\n",
    "\n",
    "    # Manipulate the string to return desired format\n",
    "    mars_weather = mars_weather[8:].capitalize().replace(\"\\n\",\"; \")\n",
    "    \n",
    "except:\n",
    "    mars_weather = \"N/A\"\n",
    "    print(\"Weather Report not available.\")\n",
    "\n",
    "# Sanity-check\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scrape _space-facts.com_ to obtain Mars parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(url_dict[\"space-facts\"])\n",
    "\n",
    "# sleep for 10 sec while the page loads\n",
    "time.sleep(10)\n",
    "\n",
    "mars_table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mars_table = mars_table.rename(columns = {0: 'Parameter', 1:\"Value\"})\n",
    "    mars_table = mars_table.set_index('Parameter')\n",
    "except:\n",
    "    print('Columns already renamed')\n",
    "    \n",
    "mars_table_html = mars_table.to_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scrape _astrogeology.com_ for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser('chrome', headless=False)\n",
    "browser.visit(url_dict[\"astrogeology\"])\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "items = soup.find_all(\"div\", class_=\"item\")\n",
    "\n",
    "mars_hemisphere_image_urls = []\n",
    "\n",
    "for item in items:    \n",
    "    full_url = url_dict['astrogeology'].split('/search')[0]+item.a['href']    \n",
    "    \n",
    "    browser.visit(full_url)\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = bs(html,'html.parser')\n",
    "    \n",
    "    image_url = soup.find(\"div\", class_=\"downloads\")\n",
    "    \n",
    "    mars_hemisphere_image_urls.append({\"title\": item.h3.text.split(' Enhanced')[0],\n",
    "   \"img_url\": image_url.li.a['href']})\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_hemisphere_image_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
